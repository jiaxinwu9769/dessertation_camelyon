Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).

Train Dataset Analysis:
Number of Samples: 262144
Label Distribution: {0.0: 131072, 1.0: 131072}
Number of Duplicates: 42119

Validation Dataset Analysis:
Number of Samples: 32768
Label Distribution: {0.0: 16399, 1.0: 16369}
Number of Duplicates: 4660

Test Dataset Analysis:
Number of Samples: 32768
Label Distribution: {0.0: 16391, 1.0: 16377}
Number of Duplicates: 3385

Compare dataset statistics
Train Dataset - Mean: 0.6436, Std: 0.2038
Validation Dataset - Mean: 0.6401, Std: 0.2088
Test Dataset - Mean: 0.6311, Std: 0.2069



MobileNetV3 structure:

backbone: MobileNetV3
  features: Sequential
    0: Conv2dNormActivation
      0: Conv2d
      1: BatchNorm2d
      2: Hardswish
    1: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    2: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    3: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    4: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    5: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    6: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    7: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    8: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    9: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    10: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    11: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    12: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    13: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    14: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    15: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    16: Conv2dNormActivation
      0: Conv2d
      1: BatchNorm2d
      2: Hardswish
  avgpool: AdaptiveAvgPool2d
  classifier: Sequential
    0: Linear
    1: Hardswish
    2: Dropout
    3: Sequential
      0: Linear
      1: ReLU
      2: Dropout
      3: Linear
      4: Sigmoid

ShuffleNetV2 structure:

backbone: ShuffleNetV2
  conv1: Sequential
    0: Conv2d
    1: BatchNorm2d
    2: ReLU
  maxpool: MaxPool2d
  stage2: Sequential
    0: InvertedResidual
      branch1: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: Conv2d
        3: BatchNorm2d
        4: ReLU
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    1: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    2: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    3: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
  stage3: Sequential
    0: InvertedResidual
      branch1: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: Conv2d
        3: BatchNorm2d
        4: ReLU
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    1: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    2: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    3: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    4: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    5: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    6: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    7: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
  stage4: Sequential
    0: InvertedResidual
      branch1: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: Conv2d
        3: BatchNorm2d
        4: ReLU
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    1: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    2: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    3: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
  conv5: Sequential
    0: Conv2d
    1: BatchNorm2d
    2: ReLU
  fc: Sequential
    0: Linear
    1: ReLU
    2: Dropout
    3: Linear
    4: Sigmoid

EfficientNet structure:

backbone: Sequential
  0: EfficientNet
    features: Sequential
      0: Conv2dNormActivation
        0: Conv2d
        1: BatchNorm2d
        2: SiLU
      1: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            2: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      2: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      3: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      4: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        2: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      5: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        2: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      6: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        2: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        3: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      7: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      8: Conv2dNormActivation
        0: Conv2d
        1: BatchNorm2d
        2: SiLU
    avgpool: AdaptiveAvgPool2d
    classifier: Sequential
      0: Linear
      1: ReLU
      2: Dropout
      3: Linear
  1: Sigmoid

ResNet18 structure:

backbone: ResNet
  conv1: Conv2d
  bn1: BatchNorm2d
  relu: ReLU
  maxpool: Identity
  layer1: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  layer2: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
      downsample: Sequential
        0: Conv2d
        1: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  layer3: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
      downsample: Sequential
        0: Conv2d
        1: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  layer4: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
      downsample: Sequential
        0: Conv2d
        1: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  avgpool: AdaptiveAvgPool2d
  fc: Sequential
    0: Linear
    1: ReLU
    2: Dropout
    3: Linear
    4: Sigmoid

[Training MobileNetV3 model...]

--------------------
Epoch 0/49
train Loss: 0.2967 Acc: 0.8814 
val Loss: 0.6664 Acc: 0.6614 
--------------------
Epoch 1/49
train Loss: 0.2405 Acc: 0.9081 
val Loss: 0.4182 Acc: 0.8217 
--------------------
Epoch 2/49
train Loss: 0.2314 Acc: 0.9128 
val Loss: 0.4512 Acc: 0.7968 
--------------------
Epoch 3/49
train Loss: 0.2250 Acc: 0.9157 
val Loss: 0.9734 Acc: 0.6552 
--------------------
Epoch 4/49
train Loss: 0.2198 Acc: 0.9182 
val Loss: 0.7397 Acc: 0.6966 
--------------------
Epoch 5/49
train Loss: 0.2173 Acc: 0.9191 
val Loss: 0.5395 Acc: 0.7624 
--------------------
Epoch 6/49
train Loss: 0.2124 Acc: 0.9214 
val Loss: 0.5962 Acc: 0.7762 
--------------------
Epoch 7/49
train Loss: 0.2132 Acc: 0.9207 
val Loss: 0.3909 Acc: 0.8261 
--------------------
Epoch 8/49
train Loss: 0.2111 Acc: 0.9218 
val Loss: 0.5564 Acc: 0.7548 
--------------------
Epoch 9/49
train Loss: 0.2099 Acc: 0.9221 
val Loss: 0.5683 Acc: 0.7708 
--------------------
Epoch 10/49
train Loss: 0.1758 Acc: 0.9378 
val Loss: 0.3574 Acc: 0.8605 
--------------------
Epoch 11/49
train Loss: 0.1696 Acc: 0.9400 
val Loss: 0.3599 Acc: 0.8619 
--------------------
Epoch 12/49
train Loss: 0.1674 Acc: 0.9410 
val Loss: 0.3085 Acc: 0.8744 
--------------------
Epoch 13/49
train Loss: 0.1658 Acc: 0.9415 
val Loss: 0.5161 Acc: 0.8135 
--------------------
Epoch 14/49
train Loss: 0.1648 Acc: 0.9420 
val Loss: 0.5118 Acc: 0.8169 
--------------------
Epoch 15/49
train Loss: 0.1643 Acc: 0.9425 
val Loss: 0.3535 Acc: 0.8534 
--------------------
Epoch 16/49
train Loss: 0.1631 Acc: 0.9426 
val Loss: 0.4762 Acc: 0.8308 
--------------------
Epoch 17/49
train Loss: 0.1618 Acc: 0.9427 
val Loss: 0.4804 Acc: 0.8191 
--------------------
Epoch 18/49
train Loss: 0.1610 Acc: 0.9435 
val Loss: 0.3274 Acc: 0.8627 
--------------------
Epoch 19/49
train Loss: 0.1613 Acc: 0.9432 
val Loss: 0.4448 Acc: 0.8364 
--------------------
Epoch 20/49
train Loss: 0.1556 Acc: 0.9459 
val Loss: 0.3992 Acc: 0.8450 
--------------------
Epoch 21/49
train Loss: 0.1546 Acc: 0.9464 
val Loss: 0.3544 Acc: 0.8566 
--------------------
Epoch 22/49
train Loss: 0.1548 Acc: 0.9467 
val Loss: 0.3602 Acc: 0.8572 
--------------------
Epoch 23/49
train Loss: 0.1542 Acc: 0.9464 
val Loss: 0.3650 Acc: 0.8554 
--------------------
Epoch 24/49
train Loss: 0.1535 Acc: 0.9474 
val Loss: 0.3806 Acc: 0.8486 
--------------------
Epoch 25/49
train Loss: 0.1538 Acc: 0.9467 
val Loss: 0.3315 Acc: 0.8651 
--------------------
Epoch 26/49
train Loss: 0.1537 Acc: 0.9468 
val Loss: 0.3260 Acc: 0.8674 
--------------------
Epoch 27/49
train Loss: 0.1533 Acc: 0.9470 
Early stopping
Test Loss: 0.3483 Acc: 0.8503 AUC: 0.9235 Precision: 0.8631 Recall: 0.8324
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

[Training ShuffleNetV2 model...]

--------------------
Epoch 0/49
train Loss: 0.3051 Acc: 0.8757 
val Loss: 0.3900 Acc: 0.8307 
--------------------
Epoch 1/49
train Loss: 0.2422 Acc: 0.9053 
val Loss: 0.7201 Acc: 0.7350 
--------------------
Epoch 2/49
train Loss: 0.2268 Acc: 0.9127 
val Loss: 0.3866 Acc: 0.8134 
--------------------
Epoch 3/49
train Loss: 0.2189 Acc: 0.9164 
val Loss: 0.4238 Acc: 0.8033 
--------------------
Epoch 4/49
train Loss: 0.2148 Acc: 0.9189 
val Loss: 1.1056 Acc: 0.6314 
--------------------
Epoch 5/49
train Loss: 0.2118 Acc: 0.9196 
val Loss: 0.6024 Acc: 0.7467 
--------------------
Epoch 6/49
train Loss: 0.2114 Acc: 0.9198 
val Loss: 0.3904 Acc: 0.8280 
--------------------
Epoch 7/49
train Loss: 0.2098 Acc: 0.9206 
val Loss: 0.5531 Acc: 0.7859 
--------------------
Epoch 8/49
train Loss: 0.2089 Acc: 0.9206 
val Loss: 0.5039 Acc: 0.7865 
--------------------
Epoch 9/49
train Loss: 0.2076 Acc: 0.9218 
val Loss: 0.4537 Acc: 0.8309 
--------------------
Epoch 10/49
train Loss: 0.1684 Acc: 0.9390 
val Loss: 0.4156 Acc: 0.8330 
--------------------
Epoch 11/49
train Loss: 0.1599 Acc: 0.9425 
val Loss: 0.3317 Acc: 0.8651 
--------------------
Epoch 12/49
train Loss: 0.1560 Acc: 0.9440 
val Loss: 0.4799 Acc: 0.8159 
--------------------
Epoch 13/49
train Loss: 0.1539 Acc: 0.9450 
val Loss: 0.4043 Acc: 0.8402 
--------------------
Epoch 14/49
train Loss: 0.1523 Acc: 0.9456 
val Loss: 0.4053 Acc: 0.8420 
--------------------
Epoch 15/49
train Loss: 0.1505 Acc: 0.9466 
val Loss: 0.3653 Acc: 0.8485 
--------------------
Epoch 16/49
train Loss: 0.1495 Acc: 0.9471 
val Loss: 0.3570 Acc: 0.8587 
--------------------
Epoch 17/49
train Loss: 0.1476 Acc: 0.9478 
val Loss: 0.3958 Acc: 0.8351 
--------------------
Epoch 18/49
train Loss: 0.1476 Acc: 0.9474 
val Loss: 0.3627 Acc: 0.8476 
--------------------
Epoch 19/49
train Loss: 0.1464 Acc: 0.9481 
val Loss: 0.4787 Acc: 0.8111 
--------------------
Epoch 20/49
train Loss: 0.1387 Acc: 0.9518 
val Loss: 0.3875 Acc: 0.8465 
--------------------
Epoch 21/49
train Loss: 0.1371 Acc: 0.9522 
val Loss: 0.3919 Acc: 0.8458 
--------------------
Epoch 22/49
train Loss: 0.1356 Acc: 0.9532 
val Loss: 0.3995 Acc: 0.8436 
--------------------
Epoch 23/49
train Loss: 0.1364 Acc: 0.9531 
val Loss: 0.3796 Acc: 0.8498 
--------------------
Epoch 24/49
train Loss: 0.1352 Acc: 0.9535 
val Loss: 0.4077 Acc: 0.8400 
--------------------
Epoch 25/49
train Loss: 0.1352 Acc: 0.9531 
val Loss: 0.3840 Acc: 0.8496 
--------------------
Epoch 26/49
train Loss: 0.1346 Acc: 0.9538 
Early stopping
Test Loss: 0.5124 Acc: 0.7986 AUC: 0.8951 Precision: 0.8839 Recall: 0.6874
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

[Training EfficientNet model...]

--------------------
Epoch 0/49
train Loss: 0.2803 Acc: 0.8883 
val Loss: 0.3765 Acc: 0.8357 
--------------------
Epoch 1/49
train Loss: 0.2397 Acc: 0.9089 
val Loss: 0.3677 Acc: 0.8357 
--------------------
Epoch 2/49
train Loss: 0.2346 Acc: 0.9123 
val Loss: 0.5192 Acc: 0.7840 
--------------------
Epoch 3/49
train Loss: 0.2374 Acc: 0.9111 
val Loss: 0.4990 Acc: 0.7608 
--------------------
Epoch 4/49
train Loss: 0.2436 Acc: 0.9071 
val Loss: 0.3354 Acc: 0.8662 
--------------------
Epoch 5/49
train Loss: 0.2482 Acc: 0.9046 
val Loss: 0.4286 Acc: 0.8032 
--------------------
Epoch 6/49
train Loss: 0.2478 Acc: 0.9046 
val Loss: 0.6104 Acc: 0.7283 
--------------------
Epoch 7/49
train Loss: 0.2456 Acc: 0.9055 
val Loss: 0.3813 Acc: 0.8353 
--------------------
Epoch 8/49
train Loss: 0.2446 Acc: 0.9061 
val Loss: 0.4004 Acc: 0.8291 
--------------------
Epoch 9/49
train Loss: 0.2435 Acc: 0.9065 
val Loss: 0.4075 Acc: 0.8183 
--------------------
Epoch 10/49
train Loss: 0.2019 Acc: 0.9254 
val Loss: 0.3643 Acc: 0.8517 
--------------------
Epoch 11/49
train Loss: 0.1945 Acc: 0.9289 
val Loss: 0.3417 Acc: 0.8600 
--------------------
Epoch 12/49
train Loss: 0.1927 Acc: 0.9297 
val Loss: 0.3604 Acc: 0.8544 
--------------------
Epoch 13/49
train Loss: 0.1915 Acc: 0.9300 
val Loss: 0.3474 Acc: 0.8522 
--------------------
Epoch 14/49
train Loss: 0.1907 Acc: 0.9306 
val Loss: 0.3623 Acc: 0.8514 
--------------------
Epoch 15/49
train Loss: 0.1902 Acc: 0.9305 
val Loss: 0.3434 Acc: 0.8605 
--------------------
Epoch 16/49
train Loss: 0.1893 Acc: 0.9312 
val Loss: 0.3400 Acc: 0.8595 
--------------------
Epoch 17/49
train Loss: 0.1888 Acc: 0.9314 
val Loss: 0.3380 Acc: 0.8640 
--------------------
Epoch 18/49
train Loss: 0.1878 Acc: 0.9315 
val Loss: 0.3330 Acc: 0.8559 
--------------------
Epoch 19/49
train Loss: 0.1876 Acc: 0.9317 
val Loss: 0.3297 Acc: 0.8662 
--------------------
Epoch 20/49
train Loss: 0.1796 Acc: 0.9353 
val Loss: 0.3538 Acc: 0.8571 
--------------------
Epoch 21/49
train Loss: 0.1781 Acc: 0.9364 
val Loss: 0.3505 Acc: 0.8566 
--------------------
Epoch 22/49
train Loss: 0.1780 Acc: 0.9360 
val Loss: 0.3567 Acc: 0.8531 
--------------------
Epoch 23/49
train Loss: 0.1776 Acc: 0.9361 
val Loss: 0.3355 Acc: 0.8641 
--------------------
Epoch 24/49
train Loss: 0.1772 Acc: 0.9366 
val Loss: 0.3634 Acc: 0.8524 
--------------------
Epoch 25/49
train Loss: 0.1769 Acc: 0.9367 
val Loss: 0.3599 Acc: 0.8542 
--------------------
Epoch 26/49
train Loss: 0.1756 Acc: 0.9368 
val Loss: 0.3862 Acc: 0.8445 
--------------------
Epoch 27/49
train Loss: 0.1771 Acc: 0.9366 
val Loss: 0.3735 Acc: 0.8477 
--------------------
Epoch 28/49
train Loss: 0.1763 Acc: 0.9368 
val Loss: 0.3769 Acc: 0.8463 
--------------------
Epoch 29/49
train Loss: 0.1762 Acc: 0.9370 
val Loss: 0.3588 Acc: 0.8532 
--------------------
Epoch 30/49
train Loss: 0.1750 Acc: 0.9374 
val Loss: 0.3722 Acc: 0.8482 
--------------------
Epoch 31/49
train Loss: 0.1749 Acc: 0.9372 
val Loss: 0.3531 Acc: 0.8567 
--------------------
Epoch 32/49
train Loss: 0.1748 Acc: 0.9374 
val Loss: 0.3741 Acc: 0.8470 
--------------------
Epoch 33/49
train Loss: 0.1757 Acc: 0.9371 
val Loss: 0.3512 Acc: 0.8564 
--------------------
Epoch 34/49
train Loss: 0.1752 Acc: 0.9370 
val Loss: 0.3518 Acc: 0.8571 
--------------------
Epoch 35/49
train Loss: 0.1749 Acc: 0.9375 
val Loss: 0.3600 Acc: 0.8552 
--------------------
Epoch 36/49
train Loss: 0.1751 Acc: 0.9374 
val Loss: 0.3580 Acc: 0.8550 
--------------------
Epoch 37/49
train Loss: 0.1752 Acc: 0.9368 
val Loss: 0.3739 Acc: 0.8481 
--------------------
Epoch 38/49
train Loss: 0.1752 Acc: 0.9370 
Early stopping
Test Loss: 0.5517 Acc: 0.7939 AUC: 0.8737 Precision: 0.8190 Recall: 0.7545
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

[Training ResNet18 model...]

--------------------
Epoch 0/49
train Loss: 0.3039 Acc: 0.8795 
val Loss: 0.5353 Acc: 0.8076 
--------------------
Epoch 1/49
train Loss: 0.2450 Acc: 0.9070 
val Loss: 0.4042 Acc: 0.8486 
--------------------
Epoch 2/49
train Loss: 0.2286 Acc: 0.9142 
val Loss: 0.4937 Acc: 0.8044 
--------------------
Epoch 3/49
train Loss: 0.2149 Acc: 0.9198 
val Loss: 0.4700 Acc: 0.7968 
--------------------
Epoch 4/49
train Loss: 0.2068 Acc: 0.9237 
val Loss: 1.2400 Acc: 0.6695 
--------------------
Epoch 5/49
train Loss: 0.2041 Acc: 0.9249 
val Loss: 0.6895 Acc: 0.7066 
--------------------
Epoch 6/49
train Loss: 0.2002 Acc: 0.9265 
val Loss: 0.6332 Acc: 0.7510 
--------------------
Epoch 7/49
train Loss: 0.1983 Acc: 0.9272 
val Loss: 0.3332 Acc: 0.8572 
--------------------
Epoch 8/49
train Loss: 0.1966 Acc: 0.9279 
val Loss: 0.6294 Acc: 0.7289 
--------------------
Epoch 9/49
train Loss: 0.1972 Acc: 0.9274 
val Loss: 0.4312 Acc: 0.8250 
--------------------
Epoch 10/49
train Loss: 0.1561 Acc: 0.9454 
val Loss: 0.3793 Acc: 0.8445 
--------------------
Epoch 11/49
train Loss: 0.1466 Acc: 0.9491 
val Loss: 0.3780 Acc: 0.8425 
--------------------
Epoch 12/49
train Loss: 0.1419 Acc: 0.9512 
val Loss: 0.3559 Acc: 0.8559 
--------------------
Epoch 13/49
train Loss: 0.1373 Acc: 0.9532 
val Loss: 0.3355 Acc: 0.8653 
--------------------
Epoch 14/49
train Loss: 0.1354 Acc: 0.9536 
val Loss: 0.4514 Acc: 0.8304 
--------------------
Epoch 15/49
train Loss: 0.1333 Acc: 0.9544 
val Loss: 0.3998 Acc: 0.8382 
--------------------
Epoch 16/49
train Loss: 0.1298 Acc: 0.9560 
val Loss: 0.3457 Acc: 0.8586 
--------------------
Epoch 17/49
train Loss: 0.1283 Acc: 0.9566 
val Loss: 0.4552 Acc: 0.8302 
--------------------
Epoch 18/49
train Loss: 0.1264 Acc: 0.9572 
val Loss: 0.3711 Acc: 0.8520 
--------------------
Epoch 19/49
train Loss: 0.1248 Acc: 0.9579 
val Loss: 0.4277 Acc: 0.8332 
--------------------
Epoch 20/49
train Loss: 0.1126 Acc: 0.9633 
val Loss: 0.3865 Acc: 0.8526 
--------------------
Epoch 21/49
train Loss: 0.1099 Acc: 0.9642 
val Loss: 0.3934 Acc: 0.8485 
--------------------
Epoch 22/49
train Loss: 0.1088 Acc: 0.9647 
val Loss: 0.3983 Acc: 0.8470 
--------------------
Epoch 23/49
train Loss: 0.1082 Acc: 0.9648 
val Loss: 0.3872 Acc: 0.8513 
--------------------
Epoch 24/49
train Loss: 0.1072 Acc: 0.9651 
val Loss: 0.3827 Acc: 0.8520 
--------------------
Epoch 25/49
train Loss: 0.1068 Acc: 0.9655 
val Loss: 0.3808 Acc: 0.8577 
--------------------
Epoch 26/49
train Loss: 0.1069 Acc: 0.9651 
val Loss: 0.3700 Acc: 0.8598 
--------------------
Epoch 27/49
train Loss: 0.1052 Acc: 0.9659 
val Loss: 0.3797 Acc: 0.8568 
--------------------
Epoch 28/49
train Loss: 0.1050 Acc: 0.9660 
Early stopping
Test Loss: 0.3344 Acc: 0.8684 AUC: 0.9390 Precision: 0.9277 Recall: 0.7989
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

MobileNetV3 Results:
Test Loss: 0.34826172643806785
Test Accuracy: 0.850250244140625
Test AUC: 0.9234643923109592
Precision: 0.8630665991390225
Recall: 0.832447945289125
Total Parameters: 4858417
Inference Time per Image: 0.00020732380653498694
FLOPs: 45791928.0

ShuffleNetV2 Results:
Test Loss: 0.5123586824629456
Test Accuracy: 0.798614501953125
Test AUC: 0.8951044226442155
Precision: 0.8838724874371859
Recall: 0.6873664285278134
Total Parameters: 1778917
Inference Time per Image: 0.0002038793172687292
FLOPs: 28385272.0

EfficientNet Results:
Test Loss: 0.5517279900377616
Test Accuracy: 0.7939453125
Test AUC: 0.8736869313219922
Precision: 0.8189832305958773
Recall: 0.7544727361543628
Total Parameters: 4663933
Inference Time per Image: 0.00022968503617448732
FLOPs: 77192256.0

ResNet18 Results:
Test Loss: 0.33435845785425045
Test Accuracy: 0.868408203125
Test AUC: 0.9389904123191914
Precision: 0.9277458696731192
Recall: 0.7989253220980643
Total Parameters: 11432001
Inference Time per Image: 0.0008471854744129814
FLOPs: 5021213696.0
Training and evaluation completed, and plots are saved as files.
