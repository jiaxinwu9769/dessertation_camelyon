
Train Dataset Analysis:
Number of Samples: 262144
Label Distribution: {0.0: 131072, 1.0: 131072}
Number of Duplicates: 42119

Validation Dataset Analysis:
Number of Samples: 32768
Label Distribution: {0.0: 16399, 1.0: 16369}
Number of Duplicates: 4660

Test Dataset Analysis:
Number of Samples: 32768
Label Distribution: {0.0: 16391, 1.0: 16377}
Number of Duplicates: 3385

Compare dataset statistics
Train Dataset - Mean: 0.6436, Std: 0.2038
Validation Dataset - Mean: 0.6401, Std: 0.2088
Test Dataset - Mean: 0.6311, Std: 0.2069



MobileNetV3 structure:

backbone: MobileNetV3
  features: Sequential
    0: Conv2dNormActivation
      0: Conv2d
      1: BatchNorm2d
      2: Hardswish
    1: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    2: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    3: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    4: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    5: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    6: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: ReLU
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    7: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    8: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    9: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    10: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    11: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    12: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    13: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    14: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    15: InvertedResidual
      block: Sequential
        0: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        1: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
          2: Hardswish
        2: SqueezeExcitation
          avgpool: AdaptiveAvgPool2d
          fc1: Conv2d
          fc2: Conv2d
          activation: ReLU
          scale_activation: Hardsigmoid
        3: Conv2dNormActivation
          0: Conv2d
          1: BatchNorm2d
    16: Conv2dNormActivation
      0: Conv2d
      1: BatchNorm2d
      2: Hardswish
  avgpool: AdaptiveAvgPool2d
  classifier: Sequential
    0: Linear
    1: Hardswish
    2: Dropout
    3: Sequential
      0: Linear
      1: ReLU
      2: Dropout
      3: Linear
      4: Sigmoid

ShuffleNetV2 structure:

backbone: ShuffleNetV2
  conv1: Sequential
    0: Conv2d
    1: BatchNorm2d
    2: ReLU
  maxpool: MaxPool2d
  stage2: Sequential
    0: InvertedResidual
      branch1: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: Conv2d
        3: BatchNorm2d
        4: ReLU
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    1: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    2: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    3: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
  stage3: Sequential
    0: InvertedResidual
      branch1: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: Conv2d
        3: BatchNorm2d
        4: ReLU
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    1: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    2: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    3: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    4: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    5: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    6: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    7: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
  stage4: Sequential
    0: InvertedResidual
      branch1: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: Conv2d
        3: BatchNorm2d
        4: ReLU
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    1: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    2: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
    3: InvertedResidual
      branch1: Sequential
      branch2: Sequential
        0: Conv2d
        1: BatchNorm2d
        2: ReLU
        3: Conv2d
        4: BatchNorm2d
        5: Conv2d
        6: BatchNorm2d
        7: ReLU
  conv5: Sequential
    0: Conv2d
    1: BatchNorm2d
    2: ReLU
  fc: Sequential
    0: Linear
    1: ReLU
    2: Dropout
    3: Linear
    4: Sigmoid

EfficientNet structure:

backbone: Sequential
  0: EfficientNet
    features: Sequential
      0: Conv2dNormActivation
        0: Conv2d
        1: BatchNorm2d
        2: SiLU
      1: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            2: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      2: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      3: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      4: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        2: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
      5: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        2: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      6: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        1: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        2: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
        3: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      7: Sequential
        0: MBConv
          block: Sequential
            0: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            1: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
              2: SiLU
            2: SqueezeExcitation
              avgpool: AdaptiveAvgPool2d
              fc1: Conv2d
              fc2: Conv2d
              activation: SiLU
              scale_activation: Sigmoid
            3: Conv2dNormActivation
              0: Conv2d
              1: BatchNorm2d
          stochastic_depth: StochasticDepth
      8: Conv2dNormActivation
        0: Conv2d
        1: BatchNorm2d
        2: SiLU
    avgpool: AdaptiveAvgPool2d
    classifier: Sequential
      0: Linear
      1: ReLU
      2: Dropout
      3: Linear
  1: Sigmoid

ResNet18 structure:

backbone: ResNet
  conv1: Conv2d
  bn1: BatchNorm2d
  relu: ReLU
  maxpool: Identity
  layer1: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  layer2: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
      downsample: Sequential
        0: Conv2d
        1: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  layer3: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
      downsample: Sequential
        0: Conv2d
        1: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  layer4: Sequential
    0: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
      downsample: Sequential
        0: Conv2d
        1: BatchNorm2d
    1: BasicBlock
      conv1: Conv2d
      bn1: BatchNorm2d
      relu: ReLU
      conv2: Conv2d
      bn2: BatchNorm2d
  avgpool: AdaptiveAvgPool2d
  fc: Sequential
    0: Linear
    1: ReLU
    2: Dropout
    3: Linear
    4: Sigmoid

[Training MobileNetV3 model...]

--------------------
Epoch 0/49
train Loss: 0.2928 Acc: 0.8835 
val Loss: 2.0048 Acc: 0.6167 
--------------------
Epoch 1/49
train Loss: 0.2373 Acc: 0.9101 
val Loss: 0.5612 Acc: 0.7555 
--------------------
Epoch 2/49
train Loss: 0.2254 Acc: 0.9164 
val Loss: 0.7178 Acc: 0.7390 
--------------------
Epoch 3/49
train Loss: 0.2200 Acc: 0.9186 
val Loss: 0.4892 Acc: 0.7714 
--------------------
Epoch 4/49
train Loss: 0.2159 Acc: 0.9204 
val Loss: 0.8945 Acc: 0.7032 
--------------------
Epoch 5/49
train Loss: 0.2146 Acc: 0.9197 
val Loss: 0.4995 Acc: 0.7921 
--------------------
Epoch 6/49
train Loss: 0.2116 Acc: 0.9222 
val Loss: 1.4932 Acc: 0.5821 
--------------------
Epoch 7/49
train Loss: 0.2104 Acc: 0.9228 
val Loss: 0.6590 Acc: 0.7621 
--------------------
Epoch 8/49
train Loss: 0.2088 Acc: 0.9229 
val Loss: 0.7958 Acc: 0.6783 
--------------------
Epoch 9/49
train Loss: 0.2064 Acc: 0.9243 
val Loss: 0.8436 Acc: 0.6951 
--------------------
Epoch 10/49
train Loss: 0.1735 Acc: 0.9388 
val Loss: 0.3141 Acc: 0.8717 
--------------------
Epoch 11/49
train Loss: 0.1673 Acc: 0.9409 
val Loss: 0.3725 Acc: 0.8492 
--------------------
Epoch 12/49
train Loss: 0.1663 Acc: 0.9412 
val Loss: 0.3061 Acc: 0.8743 
--------------------
Epoch 13/49
train Loss: 0.1640 Acc: 0.9423 
val Loss: 0.3899 Acc: 0.8478 
--------------------
Epoch 14/49
train Loss: 0.1628 Acc: 0.9427 
val Loss: 0.3395 Acc: 0.8668 
--------------------
Epoch 15/49
train Loss: 0.1623 Acc: 0.9430 
val Loss: 0.3587 Acc: 0.8598 
--------------------
Epoch 16/49
train Loss: 0.1616 Acc: 0.9432 
val Loss: 0.3432 Acc: 0.8596 
--------------------
Epoch 17/49
train Loss: 0.1607 Acc: 0.9436 
val Loss: 0.3820 Acc: 0.8426 
--------------------
Epoch 18/49
train Loss: 0.1599 Acc: 0.9442 
val Loss: 0.3749 Acc: 0.8480 
--------------------
Epoch 19/49
train Loss: 0.1602 Acc: 0.9437 
val Loss: 0.3121 Acc: 0.8689 
--------------------
Epoch 20/49
train Loss: 0.1541 Acc: 0.9459 
val Loss: 0.3777 Acc: 0.8508 
--------------------
Epoch 21/49
train Loss: 0.1531 Acc: 0.9469 
val Loss: 0.3419 Acc: 0.8615 
--------------------
Epoch 22/49
train Loss: 0.1537 Acc: 0.9468 
val Loss: 0.3486 Acc: 0.8592 
--------------------
Epoch 23/49
train Loss: 0.1536 Acc: 0.9466 
val Loss: 0.3730 Acc: 0.8537 
--------------------
Epoch 24/49
train Loss: 0.1525 Acc: 0.9472 
val Loss: 0.3470 Acc: 0.8606 
--------------------
Epoch 25/49
train Loss: 0.1526 Acc: 0.9470 
val Loss: 0.3467 Acc: 0.8608 
--------------------
Epoch 26/49
train Loss: 0.1522 Acc: 0.9470 
val Loss: 0.3688 Acc: 0.8534 
--------------------
Epoch 27/49
train Loss: 0.1520 Acc: 0.9472 
val Loss: 0.3586 Acc: 0.8573 
--------------------
Epoch 28/49
train Loss: 0.1512 Acc: 0.9476 
val Loss: 0.3242 Acc: 0.8670 
--------------------
Epoch 29/49
train Loss: 0.1522 Acc: 0.9468 
val Loss: 0.3691 Acc: 0.8543 
--------------------
Epoch 30/49
train Loss: 0.1512 Acc: 0.9477 
val Loss: 0.3410 Acc: 0.8625 
--------------------
Epoch 31/49
train Loss: 0.1510 Acc: 0.9474 
val Loss: 0.3427 Acc: 0.8622 
--------------------
Epoch 32/49
train Loss: 0.1511 Acc: 0.9478 
val Loss: 0.3446 Acc: 0.8614 
--------------------
Epoch 33/49
train Loss: 0.1509 Acc: 0.9477 
val Loss: 0.3448 Acc: 0.8619 
--------------------
Epoch 34/49
train Loss: 0.1509 Acc: 0.9476 
Early stopping
Test Loss: 0.4140 Acc: 0.8404 AUC: 0.9112 Precision: 0.9281 Recall: 0.7377
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

[Training ShuffleNetV2 model...]

--------------------
Epoch 0/49
train Loss: 0.3130 Acc: 0.8704 
val Loss: 0.4331 Acc: 0.7971 
--------------------
Epoch 1/49
train Loss: 0.2507 Acc: 0.9027 
val Loss: 0.3618 Acc: 0.8475 
--------------------
Epoch 2/49
train Loss: 0.2308 Acc: 0.9113 
val Loss: 0.4885 Acc: 0.8057 
--------------------
Epoch 3/49
train Loss: 0.2233 Acc: 0.9148 
val Loss: 0.3782 Acc: 0.8470 
--------------------
Epoch 4/49
train Loss: 0.2187 Acc: 0.9168 
val Loss: 0.4875 Acc: 0.8049 
--------------------
Epoch 5/49
train Loss: 0.2152 Acc: 0.9182 
val Loss: 0.5200 Acc: 0.7826 
--------------------
Epoch 6/49
train Loss: 0.2158 Acc: 0.9178 
val Loss: 0.3279 Acc: 0.8606 
--------------------
Epoch 7/49
train Loss: 0.2115 Acc: 0.9198 
val Loss: 0.4842 Acc: 0.7785 
--------------------
Epoch 8/49
train Loss: 0.2093 Acc: 0.9210 
val Loss: 0.4352 Acc: 0.8232 
--------------------
Epoch 9/49
train Loss: 0.2074 Acc: 0.9216 
val Loss: 0.3282 Acc: 0.8620 
--------------------
Epoch 10/49
train Loss: 0.1686 Acc: 0.9390 
val Loss: 0.3528 Acc: 0.8535 
--------------------
Epoch 11/49
train Loss: 0.1596 Acc: 0.9426 
val Loss: 0.3456 Acc: 0.8504 
--------------------
Epoch 12/49
train Loss: 0.1555 Acc: 0.9443 
val Loss: 0.4173 Acc: 0.8373 
--------------------
Epoch 13/49
train Loss: 0.1536 Acc: 0.9452 
val Loss: 0.3670 Acc: 0.8469 
--------------------
Epoch 14/49
train Loss: 0.1518 Acc: 0.9456 
val Loss: 0.3801 Acc: 0.8453 
--------------------
Epoch 15/49
train Loss: 0.1502 Acc: 0.9467 
val Loss: 0.5282 Acc: 0.7937 
--------------------
Epoch 16/49
train Loss: 0.1486 Acc: 0.9469 
val Loss: 0.3084 Acc: 0.8663 
--------------------
Epoch 17/49
train Loss: 0.1473 Acc: 0.9477 
val Loss: 0.3566 Acc: 0.8488 
--------------------
Epoch 18/49
train Loss: 0.1470 Acc: 0.9481 
val Loss: 0.2983 Acc: 0.8746 
--------------------
Epoch 19/49
train Loss: 0.1458 Acc: 0.9484 
val Loss: 0.4607 Acc: 0.8240 
--------------------
Epoch 20/49
train Loss: 0.1385 Acc: 0.9518 
val Loss: 0.3504 Acc: 0.8553 
--------------------
Epoch 21/49
train Loss: 0.1361 Acc: 0.9527 
val Loss: 0.3680 Acc: 0.8511 
--------------------
Epoch 22/49
train Loss: 0.1359 Acc: 0.9528 
val Loss: 0.3931 Acc: 0.8423 
--------------------
Epoch 23/49
train Loss: 0.1352 Acc: 0.9529 
val Loss: 0.4054 Acc: 0.8360 
--------------------
Epoch 24/49
train Loss: 0.1350 Acc: 0.9531 
val Loss: 0.3984 Acc: 0.8408 
--------------------
Epoch 25/49
train Loss: 0.1342 Acc: 0.9536 
val Loss: 0.3676 Acc: 0.8518 
--------------------
Epoch 26/49
train Loss: 0.1351 Acc: 0.9530 
val Loss: 0.3716 Acc: 0.8514 
--------------------
Epoch 27/49
train Loss: 0.1346 Acc: 0.9533 
val Loss: 0.3656 Acc: 0.8516 
--------------------
Epoch 28/49
train Loss: 0.1341 Acc: 0.9537 
val Loss: 0.3836 Acc: 0.8454 
--------------------
Epoch 29/49
train Loss: 0.1343 Acc: 0.9532 
val Loss: 0.3528 Acc: 0.8539 
--------------------
Epoch 30/49
train Loss: 0.1332 Acc: 0.9540 
val Loss: 0.3730 Acc: 0.8472 
--------------------
Epoch 31/49
train Loss: 0.1325 Acc: 0.9542 
val Loss: 0.3700 Acc: 0.8504 
--------------------
Epoch 32/49
train Loss: 0.1322 Acc: 0.9543 
val Loss: 0.3817 Acc: 0.8469 
--------------------
Epoch 33/49
train Loss: 0.1323 Acc: 0.9542 
Early stopping
Test Loss: 0.5052 Acc: 0.8124 AUC: 0.8910 Precision: 0.8793 Recall: 0.7241
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

[Training EfficientNet model...]

--------------------
Epoch 0/49
train Loss: 0.2887 Acc: 0.8853 
val Loss: 0.5957 Acc: 0.6672 
--------------------
Epoch 1/49
train Loss: 0.2448 Acc: 0.9072 
val Loss: 0.4337 Acc: 0.8095 
--------------------
Epoch 2/49
train Loss: 0.2450 Acc: 0.9074 
val Loss: 0.7780 Acc: 0.6417 
--------------------
Epoch 3/49
train Loss: 0.2462 Acc: 0.9064 
val Loss: 0.4340 Acc: 0.8123 
--------------------
Epoch 4/49
train Loss: 0.2525 Acc: 0.9035 
val Loss: 0.4252 Acc: 0.8159 
--------------------
Epoch 5/49
train Loss: 0.2576 Acc: 0.9006 
val Loss: 0.4204 Acc: 0.8082 
--------------------
Epoch 6/49
train Loss: 0.2574 Acc: 0.9003 
val Loss: 1.1019 Acc: 0.5913 
--------------------
Epoch 7/49
train Loss: 0.2557 Acc: 0.9011 
val Loss: 0.9398 Acc: 0.6701 
--------------------
Epoch 8/49
train Loss: 0.2535 Acc: 0.9021 
val Loss: 0.3770 Acc: 0.8174 
--------------------
Epoch 9/49
train Loss: 0.2515 Acc: 0.9029 
val Loss: 0.7382 Acc: 0.7299 
--------------------
Epoch 10/49
train Loss: 0.2123 Acc: 0.9218 
val Loss: 0.3859 Acc: 0.8333 
--------------------
Epoch 11/49
train Loss: 0.2058 Acc: 0.9242 
val Loss: 0.3408 Acc: 0.8516 
--------------------
Epoch 12/49
train Loss: 0.2033 Acc: 0.9256 
val Loss: 0.3505 Acc: 0.8483 
--------------------
Epoch 13/49
train Loss: 0.2030 Acc: 0.9261 
val Loss: 0.3704 Acc: 0.8315 
--------------------
Epoch 14/49
train Loss: 0.2010 Acc: 0.9265 
val Loss: 0.3273 Acc: 0.8598 
--------------------
Epoch 15/49
train Loss: 0.2014 Acc: 0.9257 
val Loss: 0.3540 Acc: 0.8511 
--------------------
Epoch 16/49
train Loss: 0.2001 Acc: 0.9265 
val Loss: 0.3570 Acc: 0.8474 
--------------------
Epoch 17/49
train Loss: 0.1994 Acc: 0.9271 
val Loss: 0.4861 Acc: 0.8096 
--------------------
Epoch 18/49
train Loss: 0.1996 Acc: 0.9267 
val Loss: 0.3297 Acc: 0.8566 
--------------------
Epoch 19/49
train Loss: 0.2000 Acc: 0.9266 
val Loss: 0.4032 Acc: 0.8357 
--------------------
Epoch 20/49
train Loss: 0.1923 Acc: 0.9308 
val Loss: 0.3448 Acc: 0.8554 
--------------------
Epoch 21/49
train Loss: 0.1905 Acc: 0.9308 
val Loss: 0.3593 Acc: 0.8520 
--------------------
Epoch 22/49
train Loss: 0.1903 Acc: 0.9315 
val Loss: 0.3838 Acc: 0.8412 
--------------------
Epoch 23/49
train Loss: 0.1902 Acc: 0.9308 
val Loss: 0.3584 Acc: 0.8500 
--------------------
Epoch 24/49
train Loss: 0.1900 Acc: 0.9310 
val Loss: 0.3451 Acc: 0.8553 
--------------------
Epoch 25/49
train Loss: 0.1904 Acc: 0.9308 
val Loss: 0.3612 Acc: 0.8495 
--------------------
Epoch 26/49
train Loss: 0.1901 Acc: 0.9306 
val Loss: 0.3590 Acc: 0.8498 
--------------------
Epoch 27/49
train Loss: 0.1899 Acc: 0.9313 
val Loss: 0.3560 Acc: 0.8501 
--------------------
Epoch 28/49
train Loss: 0.1892 Acc: 0.9317 
val Loss: 0.3512 Acc: 0.8543 
--------------------
Epoch 29/49
train Loss: 0.1894 Acc: 0.9313 
val Loss: 0.3414 Acc: 0.8570 
--------------------
Epoch 30/49
train Loss: 0.1884 Acc: 0.9317 
val Loss: 0.3557 Acc: 0.8514 
--------------------
Epoch 31/49
train Loss: 0.1882 Acc: 0.9317 
val Loss: 0.3532 Acc: 0.8524 
--------------------
Epoch 32/49
train Loss: 0.1888 Acc: 0.9318 
val Loss: 0.3534 Acc: 0.8524 
--------------------
Epoch 33/49
train Loss: 0.1885 Acc: 0.9317 
Early stopping
Test Loss: 0.4746 Acc: 0.8060 AUC: 0.8986 Precision: 0.9059 Recall: 0.6827
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

[Training ResNet18 model...]

--------------------
Epoch 0/49
train Loss: 0.3082 Acc: 0.8767 
val Loss: 0.3491 Acc: 0.8472 
--------------------
Epoch 1/49
train Loss: 0.2452 Acc: 0.9066 
val Loss: 0.3872 Acc: 0.8342 
--------------------
Epoch 2/49
train Loss: 0.2238 Acc: 0.9164 
val Loss: 0.9731 Acc: 0.6558 
--------------------
Epoch 3/49
train Loss: 0.2126 Acc: 0.9206 
val Loss: 0.4624 Acc: 0.7851 
--------------------
Epoch 4/49
train Loss: 0.2067 Acc: 0.9238 
val Loss: 0.6479 Acc: 0.7476 
--------------------
Epoch 5/49
train Loss: 0.2029 Acc: 0.9255 
val Loss: 0.3259 Acc: 0.8673 
--------------------
Epoch 6/49
train Loss: 0.2002 Acc: 0.9265 
val Loss: 0.4322 Acc: 0.8239 
--------------------
Epoch 7/49
train Loss: 0.1960 Acc: 0.9279 
val Loss: 0.3450 Acc: 0.8453 
--------------------
Epoch 8/49
train Loss: 0.1953 Acc: 0.9284 
val Loss: 0.5619 Acc: 0.7457 
--------------------
Epoch 9/49
train Loss: 0.1948 Acc: 0.9281 
val Loss: 0.3979 Acc: 0.8252 
--------------------
Epoch 10/49
train Loss: 0.1555 Acc: 0.9456 
val Loss: 0.3360 Acc: 0.8583 
--------------------
Epoch 11/49
train Loss: 0.1451 Acc: 0.9495 
val Loss: 0.4326 Acc: 0.8170 
--------------------
Epoch 12/49
train Loss: 0.1411 Acc: 0.9512 
val Loss: 0.3562 Acc: 0.8580 
--------------------
Epoch 13/49
train Loss: 0.1380 Acc: 0.9524 
val Loss: 0.3263 Acc: 0.8681 
--------------------
Epoch 14/49
train Loss: 0.1350 Acc: 0.9533 
val Loss: 0.4219 Acc: 0.8248 
--------------------
Epoch 15/49
train Loss: 0.1323 Acc: 0.9547 
val Loss: 0.4912 Acc: 0.8046 
--------------------
Epoch 16/49
train Loss: 0.1310 Acc: 0.9551 
val Loss: 0.3831 Acc: 0.8584 
--------------------
Epoch 17/49
train Loss: 0.1296 Acc: 0.9557 
val Loss: 0.4325 Acc: 0.8350 
--------------------
Epoch 18/49
train Loss: 0.1268 Acc: 0.9567 
val Loss: 0.5346 Acc: 0.8067 
--------------------
Epoch 19/49
train Loss: 0.1258 Acc: 0.9567 
val Loss: 0.3260 Acc: 0.8748 
--------------------
Epoch 20/49
train Loss: 0.1159 Acc: 0.9616 
val Loss: 0.3784 Acc: 0.8547 
--------------------
Epoch 21/49
train Loss: 0.1129 Acc: 0.9626 
val Loss: 0.3814 Acc: 0.8539 
--------------------
Epoch 22/49
train Loss: 0.1113 Acc: 0.9632 
val Loss: 0.3587 Acc: 0.8635 
--------------------
Epoch 23/49
train Loss: 0.1104 Acc: 0.9634 
val Loss: 0.3968 Acc: 0.8499 
--------------------
Epoch 24/49
train Loss: 0.1101 Acc: 0.9635 
val Loss: 0.3740 Acc: 0.8587 
--------------------
Epoch 25/49
train Loss: 0.1096 Acc: 0.9637 
val Loss: 0.4250 Acc: 0.8410 
--------------------
Epoch 26/49
train Loss: 0.1091 Acc: 0.9642 
val Loss: 0.3913 Acc: 0.8533 
--------------------
Epoch 27/49
train Loss: 0.1082 Acc: 0.9643 
val Loss: 0.4042 Acc: 0.8500 
--------------------
Epoch 28/49
train Loss: 0.1072 Acc: 0.9647 
val Loss: 0.4036 Acc: 0.8498 
--------------------
Epoch 29/49
train Loss: 0.1080 Acc: 0.9644 
val Loss: 0.3841 Acc: 0.8557 
--------------------
Epoch 30/49
train Loss: 0.1069 Acc: 0.9648 
val Loss: 0.3953 Acc: 0.8533 
--------------------
Epoch 31/49
train Loss: 0.1056 Acc: 0.9650 
val Loss: 0.4076 Acc: 0.8481 
--------------------
Epoch 32/49
train Loss: 0.1052 Acc: 0.9656 
val Loss: 0.3958 Acc: 0.8534 
--------------------
Epoch 33/49
train Loss: 0.1052 Acc: 0.9658 
val Loss: 0.3967 Acc: 0.8530 
--------------------
Epoch 34/49
train Loss: 0.1056 Acc: 0.9655 
Early stopping
Test Loss: 0.4830 Acc: 0.8253 AUC: 0.8953 Precision: 0.9133 Recall: 0.7188
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.

MobileNetV3 Results:
Test Loss: 0.4140179380774498
Test Accuracy: 0.840362548828125
Test AUC: 0.9111634051315741
Precision: 0.9280995544630511
Recall: 0.7377419551810466
Total Parameters: 4858417
Inference Time per Image: 0.0002113355658366345
FLOPs: 45791928.0

ShuffleNetV2 Results:
Test Loss: 0.5051690693944693
Test Accuracy: 0.81243896484375
Test AUC: 0.8910134664165223
Precision: 0.8793474230626622
Recall: 0.7240642364291384
Total Parameters: 1778917
Inference Time per Image: 0.00020857129129581153
FLOPs: 28385272.0

EfficientNet Results:
Test Loss: 0.47456230950774625
Test Accuracy: 0.80596923828125
Test AUC: 0.898637378712116
Precision: 0.9059233449477352
Recall: 0.6826647127068449
Total Parameters: 4663933
Inference Time per Image: 0.0002283535141032189
FLOPs: 77192256.0

ResNet18 Results:
Test Loss: 0.4829968049598392
Test Accuracy: 0.8253173828125
Test AUC: 0.8952965284493934
Precision: 0.9132593684537202
Recall: 0.7187519081638883
Total Parameters: 11432001
Inference Time per Image: 0.000847678238642402
FLOPs: 5021213696.0
Training and evaluation completed, and plots are saved as files.
